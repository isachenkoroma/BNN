\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage{graphicx, epsfig}
\usepackage{amsmath,mathrsfs,amsfonts,amssymb}
\usepackage{subfigure}
\usepackage{floatflt}
\usepackage{epic,ecltree}
\usepackage{mathtext}
\usepackage{fancybox}
\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage{bm}
\usepackage{multirow}
\usepackage{enumerate}
\usepackage{epstopdf}
\usepackage{multicol}
\usetheme{Copenhagen}%{Singapore}%{Warsaw}%{Warsaw}%{Darmstadt}
\usecolortheme{whale}
%\definecolor{beamer@blendedblue}{RGB}{15,120,80}
\definecolor{orangemy}{RGB}{66, 138, 183}
\newcommand{\T}{{\text{\tiny\sffamily\upshape\mdseries T}}}
\DeclareMathOperator*{\argmax}{arg\,max}
\setbeamertemplate{caption}{\raggedright\insertcaption\par}
%--------------------------------------------------------------------------------
\title[\hbox to 56mm{Bayesian Neural Networks  \hfill\insertframenumber\,/\,\inserttotalframenumber}]
{Bayesian Neural Networks}
\author[ROY team]{\\
	{\small \textbf{ROY team:} Ilya Zharikov, \\ \hspace{2.67cm}Roman Isachenko, \\ \hspace{2.61cm}Artem Bochkarev}}
\institute[SkolTech]{Skolkovo Institute of Science and Technology \\
	Deep Learning course 
	\vspace{0.3cm}
}
\date{May 25, 2017}
%--------------------------------------------------------------------------------
\begin{document}
	%--------------------------------------------------------------------------------
	\begin{frame}
		%\thispagestyle{empty}
		\titlepage
	\end{frame}
%--------------------------------------------------------------------------------
\begin{frame}{Project goal}	
	\begin{block}{Goal}
		Estimate posterior distributions of the model parameters from data
	\end{block}
\vspace{0.1cm}
\textbf{Probabilistic Programming:}
\begin{itemize}
	\item Uncertainty in predictions;
	\item Uncertainty in representations;
	\item Regularizations with priors;
	\item Transfer learning;
	\item Hierarchical Neural Networks.
\end{itemize}
	\begin{block}{Problem}
		Monte Carlo sampling is very slow for high-dimensional data
	\end{block}	
\end{frame}
%--------------------------------------------------------------------------------
\begin{frame}{Related work}
	\begin{enumerate}
		\item Salvatier J, Wiecki T.\,V., Fonnesbeck C. Probabilistic programming in Python using PyMC3. // \emph{PeerJ Computer Science}. 2016.
		\vfill
		\item Blundell C. et al. Weight Uncertainty in Neural Network // \emph{Proceedings of The 32nd International Conference on Machine Learning}. 2015.
		\vfill
		\item Kucukelbir A. et al. Automatic Differentiation Variational Inference // \emph{arXiv preprint arXiv:1603.00788}. â€“ 2017.
	\end{enumerate}
	
\end{frame}
%--------------------------------------------------------------------------------
\begin{frame}{Problem Statement}
	\begin{block}{Inference problem}
			Bayes' theorem states:
			$\displaystyle
			\mathbb{P}(\bm{\theta}\,|\,\mathbf{X}) = \frac{\mathbb{P}(\mathbf{X}\,|\,\bm{\theta}) \mathbb{P}(\bm{\theta})}{\mathbb{P}(\mathbf{X})}
			$
	\end{block}
	\begin{block}{Maximum A Posteriori (MAP) estimation}
		\vspace{-0.3cm}
	\[
	\bm{\theta}^* = \argmax_{\bm{\theta}} \left[\ln\mathbb{P}(\bm{\theta}\,|\,\mathbf{X})\right] =\argmax_{\bm{\theta}} \left[\ln\mathbb{P}(\mathbf{X}\,|\,\bm{\theta}) + \ln\mathbb{P}(\bm{\theta})\right]
	\]
	\end{block}
	\vfill
	\textbf{Monte Carlo approach:}
	\begin{itemize}
		\item Metropolis-Hastings sampling;
		\item Gibbs sampling;
		\item No-U-Turn Sampling (NUTS).
	\end{itemize}
\end{frame}
%--------------------------------------------------------------------------------
\begin{frame}{Variational Inference}
	\begin{block}{Goal}
	Approximate posterior distribution $p(\bm{\theta}|\bm{X})$ by function $q(\bm{\theta})$ from parametric family.
	\end{block}
	\begin{align*}
		&\ln p(\mathbf{X}) = \text{KL}(q||p) + \text{ELBO}(q)\\
		&\qquad\qquad\:\qquad\Updownarrow\qquad\qquad\Updownarrow\\
		&\int q(\bm{\theta}) \ln \frac{q(\bm{\theta})}{p(\bm{\theta}|\mathbf{X})} \text{d}\bm{\theta}
	\quad
		\int q(\bm{\theta}) \ln \frac{p(\mathbf{X}, \bm{\theta})}{q(\bm{\theta})} \text{d}\bm{\theta}
	\end{align*}
	\vspace{-0.3cm}
	\begin{block}{}
		\begin{center}
			Minimization of $\mathbf{\text{\textbf{KL}}\bm{(}q\bm{||}p\bm{)}}$ $\bm\Leftrightarrow$ Maximization of $\mathbf{\text{\textbf{ELBO}}\bm{(}q\bm{)}}$			
		\end{center}
	\end{block}
\end{frame}
%--------------------------------------------------------------------------------
\begin{frame}{Automatic Differentiation Variational Inference (ADVI)}

\begin{itemize}
	\item Automatic transformation of constrained variables $\bm{\zeta} = T(\bm{\theta})$; \\
	Example: $\theta \in \mathbb{R}_{+} \Rightarrow \zeta = T(\theta) = \log \theta$, then $\zeta \in \mathbb{R}.$
	\vfill
	\item $q(\bm{\zeta}) = \mathcal{N}(\bm{\mu}, \bm{\Sigma})$, where $\bm{\Sigma}$ is diagonal;\\
	\begin{block}{}
	$$\bm{\mu}^*, \bm{\Sigma}^* = \argmax_{\bm{\mu}, \bm{\Sigma}} \text{ELBO}(q)$$
	\end{block}
	\vfill
	\item Stochastic optimization;
	\vfill
	\item Reparametrization trick to apply automatic differentiation;
	\vfill
	\item Adaptive step-size.
\end{itemize}
\end{frame}
%--------------------------------------------------------------------------------
\begin{frame}{Deep Learning}	
	\begin{minipage}[t]{0.47\columnwidth}
		\textbf{Neural Networks} \\Predict values of parameters by fitting complex model on the huge dataset
		\begin{figure}
			\includegraphics[width=1\columnwidth]{pres_pics/BNN_1}
		\end{figure}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.47\columnwidth}
		\textbf{Bayesian Neural Networks} \\Predict the parameters of the weights distributions from the dataset
		\vspace{0.07cm}
		\begin{figure}
			\includegraphics[width=1\columnwidth]{pres_pics/BNN_2}
		\end{figure}
	\end{minipage}
	\vfill
	\hrulefill
	
	\scriptsize{http://bit.ly/2rMQuDq}
	
\end{frame}
%--------------------------------------------------------------------------------
\begin{frame}{Experiments}
	
	\textbf{Goals:}
	\begin{itemize}
		\item investigate influence of different priors on the predictions
		\item visualize uncertainties in predictions
		\item analyze the model behaviour
	\end{itemize}
	\vspace{0.3cm}
	\textbf{Datasets:}
	
	\begin{minipage}[t]{0.31\columnwidth}
		\begin{figure}
			\includegraphics[width=1\columnwidth]{pres_pics/circles_ex}
		\end{figure}
	\end{minipage}
	\begin{minipage}[t]{0.31\columnwidth}
		\begin{figure}
			\includegraphics[width=1\columnwidth]{pres_pics/moons_ex}
		\end{figure}
	\end{minipage}
	\begin{minipage}[t]{0.31\columnwidth}
		\begin{figure}
			\includegraphics[width=1\columnwidth]{pres_pics/mnist_ex}
		\end{figure}
	\end{minipage}
\end{frame}
%--------------------------------------------------------------------------------
\begin{frame}{Course of work}
	$\qquad\qquad\qquad\qquad\qquad\:\:\:$\textbf{Normal}$\:\:\:\,$\textbf{Laplace}$\:\:\:\,$\textbf{Cauchy}$\quad\:\:$\textbf{Flat}
	
	\begin{minipage}[t]{0.1\columnwidth}
		\vspace{0.5cm}
		\rotatebox{90}{\textbf{Hierarchical modeling}$\qquad\:\:$\textbf{Fix}}
	\end{minipage}\
	\begin{minipage}[t]{0.23\columnwidth}
	\vspace{0.65cm}
	Fixed values
	
	\vspace{1.05cm}
	
	Half-Normal
	
	\vspace{1.03cm}
	
	Half-Cauchy
	
	\vspace{1.01cm}
	
	Inverse-Gamma
	\end{minipage}
	\begin{minipage}[t]{0.62\columnwidth}
		\begin{figure}
			\includegraphics[height=6cm]{pres_pics/all.png}
		\end{figure}
	\end{minipage}
	
	
\end{frame}
%--------------------------------------------------------------------------------
\begin{frame}{Synthetic data}
	\begin{minipage}[t]{0.32\columnwidth}
	\vspace{0.9cm}
	\textbf{Prior:} Cauchy\\
	\textbf{Hyperprior:} \\Inverse-Gamma\\
	\textbf{Accuracy:} 0.735
	\end{minipage}
	\begin{minipage}[t]{0.32\columnwidth}
		\centering 
		\textbf{Posterior}\\\textbf{Probability}
		\vspace{-0.3cm}
		\begin{figure}
			\includegraphics[height=3cm]{pres_pics/circles_inv_cauchy_ppm.png}
		\end{figure}
	\end{minipage}
	\begin{minipage}[t]{0.32\columnwidth}
		\centering
		\vspace{0.06cm}
		\textbf{Uncertainty}
		\vspace{-0.15cm}
		\begin{figure}
			\includegraphics[height=3cm]{pres_pics/circles_inv_cauchy_uncert.png}
		\end{figure}
	\end{minipage}

\vfill

\begin{minipage}[t]{0.32\columnwidth}
	\vspace{0.2cm}
	\textbf{Prior:} Normal \\
	\textbf{Hyperprior:} \\Inverse-Gamma\\
	\textbf{Accuracy:} 0.851
\end{minipage}
\begin{minipage}[t]{0.32\columnwidth}
	\begin{figure}
		\includegraphics[height=3cm]{pres_pics/moons_inv_gauss_ppm.png}
	\end{figure}
\end{minipage}
\begin{minipage}[t]{0.32\columnwidth}
	\begin{figure}
		\includegraphics[height=3cm]{pres_pics/moons_inv_gauss_uncert.png}
	\end{figure}
\end{minipage}
\end{frame}
%--------------------------------------------------------------------------------
\begin{frame}{Hierarchical modelling}
	\begin{minipage}[t]{0.32\columnwidth}
	\vspace{0.6cm}
	\textbf{Hyperprior:} \\Fixed values
	\end{minipage}
	\begin{minipage}[t]{0.32\columnwidth}
		\centering
		\textbf{Prior:} Normal
		\begin{figure}
			\includegraphics[height=3cm]{pres_pics/p_gauss.png}
		\end{figure}
	\end{minipage}
	\begin{minipage}[t]{0.32\columnwidth}
		\centering
		\textbf{Prior:} Laplace
		\vspace{-0.075cm}
		\begin{figure}
			\includegraphics[height=3cm]{pres_pics/p_laplace.png}
		\end{figure}
	\end{minipage}
	
	\vfill
	
	\begin{minipage}[t]{0.32\columnwidth}
		\vspace{0.2cm}
		\textbf{Hyperprior:} \\Inverse-Gamma
	\end{minipage}
	\begin{minipage}[t]{0.32\columnwidth}
		\begin{figure}
			\includegraphics[height=3cm]{pres_pics/h_gauss.png}
		\end{figure}
	\end{minipage}
	\begin{minipage}[t]{0.32\columnwidth}
		\begin{figure}
			\includegraphics[height=3cm]{pres_pics/h_laplace.png}
		\end{figure}
	\end{minipage}
\end{frame} 
%--------------------------------------------------------------------------------
\begin{frame}{Laplace sparsity}
	\begin{figure}
		\includegraphics[width=1\columnwidth]{pres_pics/trace_laplace_w3.png}
	\end{figure}
	\begin{figure}
		\includegraphics[width=1\columnwidth]{pres_pics/trace_cauchy_w3.png}
	\end{figure}
\end{frame}
%--------------------------------------------------------------------------------
\begin{frame}{MNIST}
			\begin{figure}
		\includegraphics[width=0.6\columnwidth]{pres_pics/MNIST}
	\end{figure}
\end{frame}
%--------------------------------------------------------------------------------
\begin{frame}{MNIST}
	\begin{minipage}[t]{0.45\columnwidth}
		\begin{figure}
			\includegraphics[width=1\columnwidth]{pres_pics/mnist_variance}
		\end{figure}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.46\columnwidth}
		\begin{figure}
			\includegraphics[width=1\columnwidth]{pres_pics/mnist_eer}
		\end{figure}
	\end{minipage}

	\vfill

	\textbf{Conclusions:}
	\begin{itemize}
		\item Accuracy score: $97.7 \%$;
		\item Variance is much higher for misclassified pictures;
		\item Model is not always confident.
	\end{itemize}
\end{frame}
%--------------------------------------------------------------------------------
\begin{frame}{MNIST}
	Misclassified pictures with \textbf{zero expected error rate}:
	
	\vspace{0.5cm}
	
	\begin{minipage}[t]{0.32\columnwidth}
		\fbox{\begin{minipage}[t]{0.25\columnwidth}
			\centering
			\textbf{True}\\
			\LARGE{7}
		\end{minipage}}
		\fbox{\begin{minipage}[t]{0.57\columnwidth}
				\centering
				\textbf{Prediction}\\
				\LARGE{0}
		\end{minipage}}
		\begin{figure}
			\includegraphics[width=1\columnwidth]{pres_pics/high_conf/70}
		\end{figure}
	\end{minipage}
\hfill
	\begin{minipage}[t]{0.32\columnwidth}
		\fbox{\begin{minipage}[t]{0.25\columnwidth}
				\centering
				\textbf{True}\\
				\LARGE{9}
		\end{minipage}}
		\fbox{\begin{minipage}[t]{0.57\columnwidth}
				\centering
				\textbf{Prediction}\\
				\LARGE{7}
		\end{minipage}}
		\begin{figure}
			\includegraphics[width=1\columnwidth]{pres_pics/high_conf/97}
		\end{figure}
	\end{minipage}
\hfill
	\begin{minipage}[t]{0.32\columnwidth}
		\fbox{\begin{minipage}[t]{0.25\columnwidth}
				\centering
				\textbf{True}\\
				\LARGE{5}
		\end{minipage}}
		\fbox{\begin{minipage}[t]{0.57\columnwidth}
				\centering
				\textbf{Prediction}\\
				\LARGE{6}
		\end{minipage}}
		\begin{figure}
			\includegraphics[width=1\columnwidth]{pres_pics/high_conf/56}
		\end{figure}
	\end{minipage}
\end{frame}
%--------------------------------------------------------------------------------
\begin{frame}{MNIST}
	Pictures with the \textbf{lowest confidence}:
	
	\vspace{0.5cm}
	
	\begin{minipage}[t]{0.32\columnwidth}
		\fbox{\begin{minipage}[t]{0.25\columnwidth}
				\centering
				\textbf{True}\\
				\LARGE{4}
		\end{minipage}}
		\fbox{\begin{minipage}[t]{0.57\columnwidth}
				\centering
				\textbf{Prediction}\\
				\LARGE{0}
		\end{minipage}}
		\begin{figure}
			\includegraphics[width=1\columnwidth]{pres_pics/low_conf/40}
		\end{figure}
	\end{minipage}
\hfill
	\begin{minipage}[t]{0.32\columnwidth}
		\fbox{\begin{minipage}[t]{0.25\columnwidth}
				\centering
				\textbf{True}\\
				\LARGE{6}
		\end{minipage}}
		\fbox{\begin{minipage}[t]{0.57\columnwidth}
				\centering
				\textbf{Prediction}\\
				\LARGE{8}
		\end{minipage}}
		\begin{figure}
			\includegraphics[width=1\columnwidth]{pres_pics/low_conf/68}
		\end{figure}
	\end{minipage}
\hfill
	\begin{minipage}[t]{0.32\columnwidth}
		\fbox{\begin{minipage}[t]{0.25\columnwidth}
				\centering
				\textbf{True}\\
				\LARGE{1}
		\end{minipage}}
		\fbox{\begin{minipage}[t]{0.57\columnwidth}
				\centering
				\textbf{Prediction}\\
				\LARGE{1}
		\end{minipage}}
		\begin{figure}
			\includegraphics[width=1\columnwidth]{pres_pics/low_conf/11}
		\end{figure}
	\end{minipage}
\end{frame}

%--------------------------------------------------------------------------------
\begin{frame}{Conclusion}
	\begin{itemize}
		\item Posterior distribution helps to make conclusions about uncertainties
		\vspace{0.2cm}
		\item Variational inference allows to approximate posterior distribution for high-dimensional data
		\vspace{0.2cm}
		\item Hierarchical models have more degrees of freedom
	\end{itemize}
\end{frame}

\end{document} 