\documentclass[a4paper,14pt]{article} 
% Этот шаблон документа разработан в 2014 году 
% Данилом Фёдоровых (danil@fedorovykh.ru) 
% для использования в курсе 
% «Документы и презентации в \LaTeX», записанном НИУ ВШЭ 
% для Coursera.org: http://coursera.org/course/latex . 
% Исходная версия шаблона —- 
% https://www.writelatex.com/coursera/latex/5.3 

% В этом документе преамбула 

%%% Работа с русским языком 
\usepackage{cmap} % поиск в PDF 
\usepackage{mathtext} % русские буквы в формулах 
\usepackage[T2A]{fontenc} % кодировка 
\usepackage[cp1251]{inputenc} % кодировка исходного текста 
\usepackage[english]{babel} % локализация и переносы 
\usepackage{indentfirst} 
\frenchspacing
\usepackage[shortlabels]{enumitem}

\renewcommand{\epsilon}{\ensuremath{\varepsilon}} 
\renewcommand{\phi}{\ensuremath{\varphi}} 
\renewcommand{\kappa}{\ensuremath{\varkappa}} 
\renewcommand{\le}{\ensuremath{\leqslant}} 
\renewcommand{\leq}{\ensuremath{\leqslant}} 
\renewcommand{\ge}{\ensuremath{\geqslant}} 
\renewcommand{\geq}{\ensuremath{\geqslant}} 
\renewcommand{\emptyset}{\varnothing} 
\newcommand{\T}{{\text{\footnotesize\sffamily\upshape\mdseries T}}}


%%% Дополнительная работа с математикой 
\usepackage{amsmath,amsfonts,amssymb,amsthm,mathtools, bm} % AMS 
\usepackage{icomma} % "Умная" запятая: $0,2$ —- число, $0, 2$ —- перечисление 

%% Номера формул 
%\mathtoolsset{showonlyrefs=true} % Показывать номера только у тех формул, на которые есть \eqref{} в тексте. 
%\usepackage{leqno} % Нумереация формул слева 

%% Свои команды 
\DeclareMathOperator{\sgn}{\mathop{sgn}}
\DeclareMathOperator*{\argmin}{arg\,min}

%% Перенос знаков в формулах (по Львовскому) 
%\newcommand*{\hm}[1]{#1\nobreak\discretionary{} 
%	{\hbox{$\mathsurround=0pt #1$}}{}} 

%%% Работа с картинками 
\usepackage{graphicx} % Для вставки рисунков 
\setlength\fboxsep{3pt} % Отступ рамки \fbox{} от рисунка 
\setlength\fboxrule{1pt} % Толщина линий рамки \fbox{} 
\usepackage{wrapfig} % Обтекание рисунков текстом 
\usepackage{float}
\usepackage{subfig}

%%% Работа с таблицами 
\usepackage{array,tabularx,tabulary,booktabs} % Дополнительная работа с таблицами 
\usepackage{longtable} % Длинные таблицы 
\usepackage{multirow} % Слияние строк в таблице 

%%% Теоремы 
\theoremstyle{plain} % Это стиль по умолчанию, его можно не переопределять. 
\newtheorem{theorem}{Теорема}[section] 
\newtheorem{proposition}[theorem]{Утверждение}

\theoremstyle{definition} % "Определение" 
\newtheorem{corollary}{Следствие}[theorem] 
\newtheorem{problem}{Задача}[section] 

\theoremstyle{remark} % "Примечание" 
\newtheorem*{nonum}{Решение} 

%%% Программирование 
\usepackage{etoolbox} % логические операторы

%%% Страница 
\usepackage{extsizes} % Возможность сделать 14-й шрифт 
\usepackage{geometry} % Простой способ задавать поля 
\geometry{top=20mm} 
\geometry{bottom=25mm} 
\geometry{left=20mm} 
\geometry{right=20mm} 
% 
%\usepackage{fancyhdr} % Колонтитулы 
% \pagestyle{fancy} 
%\renewcommand{\headrulewidth}{0pt} % Толщина линейки, отчеркивающей верхний колонтитул 
% \lfoot{Нижний левый} 
% \rfoot{Нижний правый} 
% \rhead{Верхний правый} 
% \chead{Верхний в центре} 
% \lhead{Верхний левый} 
% \cfoot{Нижний в центре} % По умолчанию здесь номер страницы 

\usepackage{setspace} % Интерлиньяж 
\onehalfspacing % Интерлиньяж 1.5 
%\doublespacing % Интерлиньяж 2 
%\singlespacing % Интерлиньяж 1
\setlength{\parindent}{1cm}

\usepackage{lastpage} % Узнать, сколько всего страниц в документе. 

\usepackage{soul} % Модификаторы начертания 

\usepackage{hyperref} 
\usepackage[usenames,dvipsnames,svgnames,table,rgb]{xcolor} 
\hypersetup{ % Гиперссылки 
	unicode=true, % русские буквы в раздела PDF 
	pdftitle={Заголовок}, % Заголовок 
	pdfauthor={Автор}, % Автор 
	pdfsubject={Тема}, % Тема 
	pdfcreator={Создатель}, % Создатель 
	pdfproducer={Производитель}, % Производитель 
	pdfkeywords={keyword1} {key2} {key3}, % Ключевые слова 
	colorlinks=true, % false: ссылки в рамках; true: цветные ссылки 
	linkcolor=red, % внутренние ссылки 
	citecolor=black, % на библиографию 
	filecolor=magenta, % на файлы
	urlcolor=blue % на URL 
} 

\usepackage{csquotes} % Еще инструменты для ссылок 

%\usepackage[style=authoryear,maxcitenames=2,backend=biber,sorting=nty]{biblatex} 

\usepackage{multicol} % Несколько колонок 

\usepackage{tikz} % Работа с графикой 
\usepackage{pgfplots} 
\usepackage{pgfplotstable} 
\usepackage{bbm}
\usepackage{cite}

\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\bX}{\mathbf{X}} 

\author
{Ilya Zharikov, Roman Isachenko, Artem Bochkarev} 
\title 
{Bayesian Neural Networks}
\date{}

\begin{document} 
\maketitle
\begin{abstract}
	The recent advances in deep learning allow us to solve really hard real word problems with high accuracy. What these methods lack is interpretability of the model and uncertainty guarantees for the predictions. Bayesian framework allows us to solve both these two issues, and with recent development in variational inference technique it is possible to take the best of two worlds and implement bayesian neural network. Our project is dedicated to testing different priors for neurons weights and exploring advantages and disadvantages of bayesian approach in deep learning.
\end{abstract}

\section*{Introduction}
Deep learning is one of the most promising and quickly advancing areas of machine learning research~\cite{Goodfellow-et-al-2016}. Deep models proved to be superior to other algorithms in many fields such as image classification~\cite{krizhevsky2012imagenet}, speech recognition~\cite{dahl2012context}, unsupervised learning~\cite{le2013building} and reinforcement learning~\cite{mnih2015human}. Still, these models are mostly used as some black box in order to solve complicated problems. We often want to know not only the prediction of the network on given input, but we would like to understand the uncertainty~\cite{blundell2015weight} of the model on this object. It is also desirable to be able to do transfer learning~\cite{pan2010survey}, i.e. be able to obtain correct results on new types of data, using previous model as prior knowledge.

On the other side of the spectrum there is bayesian approach~\cite{bishop2006pattern}. The main advantage of this framework is that we operate with parameters distribution, instead of their point estimation. This allows us to calculate uncertainties in predictions and representations. Another advantage of the bayesian approach is that we are free to select prior distribution for the parameters of our model~\cite{browne2006comparison}, which may include real knowledge or some non-informative assumption. Finally, we may want to build hierarchical models because data is too complex and good priors are hard to guess.

With recent advances in computational technology and GPU it is very compelling to try mixing deep learning with bayesian framework. This would allow us to achieve the same high performance of neural network, as well give us more freedom with different prior selection and interpretability of the results using uncertainty prediction. One of the obstacles for such approach is that full bayesian inference is very memory expensive, given huge parameters number (we need to store whole distribution instead of one point). What is more important, calculation of the model evidence is intractable for high-dimensional data. MCMC sampling methods~\cite{wasserman2013all} like Metropolis-Hastings or Gibbs are too computationally expensive to perform on big models.

In our project we decided to implement alternative way for approximation of posterior probability, namely ADVI~\cite{kucukelbir2016automatic}. The main idea of the method is that we constitute true posterior with some distribution from known parametric family and then we use stochastic optimization procedure to minimize their difference.

We conducted computational experiments on several datasets. We used\\ PyMC3~\cite{salvatier2016probabilistic} and Lasagne~\cite{lasagne} for probabilistic and deep learning frameworks respectively. We used synthetic datasets as well as MNIST hand-written digits dataset. For these datasets we constructed bayesian neural network models, optimized them with ADVI, played with different priors and drew conclusions from model uncertainty.

\section*{Problem Statement}
\subsection*{Bayesian approach}
Suppose that $\bX$ is the observed data which comes from the unknown distribution $p(\bX)$. 
This distribution defines our model and is called model evidence. 
We assume that our model also contains latent variables $\bm{\theta}$. 
The likelihood function is given by conditional probability distribution $p(\bX| \bm{\theta})$. 
If we know the prior distribution $p(\bm{\theta})$ over hidden variables, we could use Bayes' theorem to derive the posterior distribution $p(\bm{\theta}| \bX)$
\begin{equation}
p(\bm{\theta}| \bX) = \frac{p(\bX| \bm{\theta}) p(\bm{\theta})}{p(\bX)},
\label{bayes_th}
\end{equation}
where the evidence function $p(\bX)$ could be obtained by integration over hidden variables
\[
p(\bX) = \int p(\bX| \bm{\theta}) p(\bm{\theta})p(\bm{\theta}).
\]
The posterior distribution shows the transformed initial prior knowledge of $\bm{\theta}$ given the observed data $\bX$. 

The main goal of bayesian inference to compute the posterior distribution. 
In some cases we could derive the posterior in the closed form formula. 
However, it is impossible for complex models where the latent variables lies in high-dimensional space. 
The problem is the denominator of~\eqref{bayes_th}, since it is obtained by integrating over all possible values of $\bm{\theta}$.

Suppose that we have the posterior distribution $p(\bm{\theta}| \bX)$, then we could use the usual Maximum A Posteriori (MAP) approach to obtain point estimation of the parameters
\[
\bm{\theta}_{\text{MAP}} = \argmax_{\bm{\theta}} \left[\ln p(\bm{\theta}|\bX)\right] =\argmax_{\bm{\theta}} \left[\ln p(\bX|\bm{\theta}) + \ln p(\bm{\theta})\right].
\]
In case of flat prior distribution $p(\bm{\theta}) = \text{const}$, this estimation coincides with the Maximum Likelihood Estimation (MLE) approach
\[
\bm{\theta}_{\text{MLE}} = \argmax_{\bm{\theta}} \left[\ln p(\bX|\bm{\theta})\right].
\]

\subsection*{Hierarchical modelling}

There is one natural addition to previously described bayesian approach, which we will use in our project extensively, namely hierarchical modelling. If we look at Bayes theorem, we realize that the prior selection might be very important for model performance. This issue is more critical if we have little data or very complex model. 

Hierarchical modelling solve this problem by allowing us to learn priors from the data itself. We consider prior distribution $p(\bm{\theta})$ of parameter $\bm{\theta}$ to be dependent on some additional hyperparameter $\alpha$. Following full bayesian approach, we need to introduce hyperprior $p(\alpha)$. Expression for posterior distribution becomes
\begin{equation*}
p(\bm{\theta}, \alpha| \bX) = \frac{p(\bX| \bm{\theta}, \alpha) p(\bm{\theta}|\alpha) p(\alpha)}{p(\bX)}
\end{equation*}
Now we don't have to guess prior distribution, we can learn it from data directly.
\subsection*{Variational inference}

One of the widely used approach to get posterior distribution is sampling methods such as Metropolis-Hastings, Gibbs, NUTS algorithms. 
The general idea behind sampling methods is to obtain procedure for generating samples from the true posterior distribution. 
Most of these methods are based on Monte Carlo Markov Chains (MCMC) procedures.
The main disadvantage of such methods is that they are very slow for high-dimensional data.

In order to deal with high-dimensional data we could use analytical deterministic approximation of the posterior. 
Variational inference approach determines the function which is the closest to the desired posterior distribution. 
The distance is measured by Kullback-Leibler divergence
\[
\text{KL}(q||p) = -\int q(y) \ln \left[\frac{p(y)}{q(y)}\right]dy.
\]
This distance function is non-negative and equals to zero iff $q \equiv p$
\[
\text{KL}(q||p) \geq 0; \quad \text{KL}(q||p) = 0 \Leftrightarrow q \equiv p.
\]
But the problem is that we don't know the true posterior and can't minimize the Kullback-Leibler divergence explicitly. 
Variational inference approach solves the equivalent task which follows from the equation:
\begin{multline*}
\ln p(\bX) = \int q(\bm{\theta}) \ln p(\bX) d \bm{\theta} = \int q(\bm{\theta}) \ln \left[ \frac{p(\bX, \bm{\theta})}{p(\bm{\theta} | \bX)} \right] d \bm{\theta} = \\
= \int q(\bm{\theta}) \ln \left[ \frac{q(\bm{\theta})p(\bX, \bm{\theta}) }{p(\bm{\theta} | \bX)q(\bm{\theta})} \right] d \bm{\theta} = 
- \int q(\bm{\theta}) \ln \left[ \frac{p(\bm{\theta} | \bX)}{q(\bm{\theta})} \right] d \bm{\theta} + \\
+ \int q(\bm{\theta}) \ln \left[ \frac{p(\bX, \bm{\theta}) }{q(\bm{\theta})} \right] d \bm{\theta} = \text{KL}(q||p) + \text{ELBO}(q).
\end{multline*}
Here, the log model evidence function is decomposed into the Kullback-Leibler divergence and the Empirical Lower BOund (ELBO) term. 
Since the left hand side is independent of $q$ function, the minimization of $\text{KL}(q||p)$ is equivalent to the maximization of $\text{ELBO}(q)$ which we can easily compute. 
\begin{equation}
q^* = \argmin_{q} \text{KL}(q||p) = \argmax_{q} \text{ELBO}(q).
\label{minKL_maxELBO}
\end{equation}
This lower bound can be expressed in the following way
\begin{align}
\nonumber \text{ELBO}(q) &= \int q(\bm{\theta}) \ln \left[ \frac{p(\bX, \bm{\theta}) }{q(\bm{\theta})} \right] d \bm{\theta}  = \\\nonumber&= \int q(\bm{\theta}) \ln p(\bX, \bm{\theta}) d \bm{\theta} - \int q(\bm{\theta}) \ln q(\bm{\theta}) d \bm{\theta} = \\
&= \mathbb{E}_{q(\bm{\theta})} \left[\ln p(\bX, \bm{\theta})  \right] - \mathbb{E}_{q(\bm{\theta})} \left[ \ln q(\bm{\theta})  \right].
\label{ELBO}
\end{align}

The joint probability distribution $p(\bX, \bm{\theta})$ is given by product of likelihood function $p(\bX)$ and prior distribution $p(\bm{\theta})$.

\subsection*{Automatic differentiation variational inference}
In~\cite{kucukelbir2016automatic} the Automatic Differentiation Variational Inference (ADVI) method was proposed to solve the problem~\eqref{minKL_maxELBO}. 

At the first stage ADVI method automatically transforms the original constrained variables $\bm{\theta}$ to the unconstrained real-valued variables $\bm{\zeta} = T(\bm{\theta})$. ADVI then defines the corresponding variational problem on the transformed variables, that is, to minimize $\text{KL}\left(q(\bm{\zeta})|| p(\bm{\zeta}| \bX)\right)$. With this transformation, all latent variables are defined on the same space. E.\,g. if some component $\theta$ of $\bm{\theta}$ are non-negative ($\theta \in \mathbb{R}_+$), then one could use the logarithm transformation $\zeta = T(\theta) = \log(\theta) \in \mathbb{R}$. 

The joint probability function for transformed latent variables equals 
\[
p(\bX, \bm{\zeta}) = p(\bX, T^{-1}(\bm{\zeta})) |\det J_{T^{-1}}(\bm{\zeta})|,
\]
where $J_{T^{-1}}(\bm{\zeta})$ is the Jacobian of the inverse $T$ transformation.

The lower bound~\eqref{ELBO} in real coordinate space is given by
\begin{equation}
	\text{ELBO}(q(\bm{\zeta})) = \mathbb{E}_{q(\bm{\zeta})} \left[ \ln p(\bX, T^{-1}(\bm{\zeta})) + \ln |\det J_{T^{-1}}(\bm{\zeta})| \right] - \mathbb{E}_{q(\bm{\zeta})} \left[ \ln q(\bm{\zeta})  \right].
	\label{ELBO_real}
\end{equation}

The next stage of the ADVI algorithm includes stochastic optimization. Firstly, we assume that $q(\bm{\zeta})$ comes from fixed parametric family. It allows us to avoid variational optimization and replace it by parameter optimization. Usually ADVI uses $\mathcal{N}(\bm{\mu}, \bm{\Sigma})$ family with diagonal $\bm{\Sigma}$. The general positive-definite form of the covariance matrix $\bm{\Sigma}$ is also possible, but it leads to excessive computations. After this parametrization the problem~\eqref{minKL_maxELBO} is equivalent to
\[
	\bm{\mu}^*, \bm{\Sigma}^* = \argmax_{\bm{\mu}, \bm{\Sigma}} \text{ELBO}(q).
\]

The solution of this problem is obtained using stochastic optimization. However, we cannot directly use automatic differentiation on the ELBO. This is because the objective function is expectation over optimized parameters. To avoid this problem one could use elliptical standardization as reparametrization trick~\cite{kingma2013auto}. We convert the Gaussian variational approximation to the standard Gaussian $\bm{\eta} = S(\bm{\zeta})$
\[
	q(\bm{\eta}) = \mathcal{N}(\bm{\eta}|0, \mathbf{I}).
\]
The standardization transforms the variational lower bound from\eqref{ELBO_real} into
\begin{multline*}
\text{ELBO}(q(\bm{\eta})) = \mathbb{E}_{ \mathcal{N}(\bm{\eta}|0, \mathbf{I})} \Bigl[ \ln p\left(\bX, T^{-1}(S^{-1}(\bm{\eta}))\right) + \\ + \ln |\det J_{T^{-1}}(S^{-1}(\bm{\eta}))| \Bigr] - \mathbb{E}_{\mathcal{N}(\bm{\eta}|0, \mathbf{I})} \left[ \ln q(\bm{\eta})  \right].
\end{multline*}
Now the expectation is independent of the optimized parameters and we can easily compute gradients over $\bm{\mu}$ and $\bm{\Sigma}$.

In the original paper a new adaptive step-size gradient sequence was proposed. This sequence is implemented in used frameworks. But it is of no interest for our research.

\subsection*{Bayesian neural networks}
ADVI was implemented for deep learning models. In our project, neural network consists of several layers of neurons, outputs $\mathbf{x}_k$ of layer $k$ depends on layer $k-1$ neurons $\mathbf{x}_{k-1}$ through some linear combination with coefficients $\mathbf{W}_{k-1}$, $\mathbf{b}_{k-1}$ and activation function $g_{k-1}$:
\begin{equation*}
	\mathbf{x}_k = g_{k-1}\left(\mathbf{W}_{k-1}\mathbf{x}_{k-1} + \mathbf{b}_{k-1}\right).
\end{equation*}
The usual deep learning approach is to use stochastic gradient descent and error backpropagation in order to fit the network parameters $\{\mathbf{W}_k$, $\mathbf{b}_k\}$, where $k$ iterates over all network layers. The activation function such as $\textit{tanh}$, $\textit{ReLU}$, $\textit{softmax}$ allows to fit non-linear models.

We implemented bayesian approach, i.e. instead of point estimation, each parameter and each output in the network is given by probability distribution, the difference between two approaches can be seen on fig.~\ref{pic:BNN}. These probability distributions correspond to the priors of the bayesian model described above. The usual models tend to overfit in the case of high-dimensional parameter space. This is exactly the case with deep neural networks, where number of estimated parameters are several millions or more. There are some techniques to avoid this problem. The most widely used  is regularization. Adding the term which restricts the model complexity allows to control parameters values. Introducing priors distributions is exactly the same procedure. The normal prior corresponds to $\ell 2$ regularization and the laplace prior~--- $\ell 1$ regularizer.
\begin{figure}[h]
	\centering
	\subfloat[Conventional neural network]{\includegraphics[width=0.47\columnwidth]{pres_pics/BNN_1}}
	\hfill
	\subfloat[Bayesian neural network]{\includegraphics[width=0.47\columnwidth]{pres_pics/BNN_2}}
	\caption{Neural networks difference}
	\label{pic:BNN}
\end{figure}


\section*{Numerical experiments}

\subsection*{Datasets}

In our research we used synthetic data for binary classification problem in non-linearly separable case and real data such as MNIST\footnote{%
	\href{https://www.kaggle.com/c/digit-recognizer/data}{MNIST dataset~--- https://www.kaggle.com/c/digit-recognizer/data}} dataset for multiclass classification problem. One could find the detailed description of used data below.

\textbf{Synthetic data}

Synthetic data consists of two-dimensional binary classification datasets (moons and circles) which are challenging, since they are non-linearly separable.

1. Moons: two interleaving half-circles (see fig.\ref{pic: ex_data} (a));

2. Circles: a large circle containing a smaller circle (see fig.\ref{pic: ex_data} (b)).


\textbf{Real data}

As for the real data MNIST dataset was selected. 
The data contains gray-scale images of hand-drawn digits, from zero through nine.

Each image is $28$ pixels in height and $28$ pixels in width, for a total of $784$ pixels in total (see fig.\ref{pic: ex_data} (c)). Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between $0$ and $255$, inclusive.

\begin{figure}[h]
	\centering
	\subfloat[Moons]{\includegraphics[width=0.32\columnwidth]{pres_pics/moons_ex}}
	\hfill
	\subfloat[Circles]{\includegraphics[width=0.32\columnwidth]{pres_pics/circles_ex}}
	\hfill
	\subfloat[MNIST]{\includegraphics[width=0.32\columnwidth]{pres_pics/mnist_ex}}
	\caption{Synthetic and real data examples}
	\label{pic: ex_data}
\end{figure}

\subsection*{Priors and hyperpriors influence}

On the following figure one could see the design of the experiment for synthetic data.

$\qquad\qquad\qquad\qquad\qquad\qquad\:\:$\textbf{Normal}$\:\:\:\:\:$\textbf{Laplace}$\:\:\:\:\:$\textbf{Cauchy}$\quad\:\:\:\:$\textbf{Flat}

\begin{minipage}[t]{0.1\columnwidth}
	\vspace{-8.0cm}
	\rotatebox{90}{\textbf{Hierarchical modelling}$\qquad\:\:\:$\textbf{Fix}}
\end{minipage}
\begin{minipage}[t]{0.23\columnwidth}
	\vspace{-7.9cm}
	Fixed values
	
	\vspace{1.5cm}
	
	Half-Normal
	
	\vspace{1.5cm}
	
	Half-Cauchy
	
	\vspace{1.4cm}
	
	Inverse-Gamma
\end{minipage}
\hfill
\begin{minipage}[t]{0.6\columnwidth}
	\includegraphics[width=0.93\columnwidth]{pres_pics/all}
\end{minipage}

Normal, laplace, cauchy and flat distributions were used as priors. We also tried hierarchical modelling approach. Fixed values of parameters, half-normal, half-cauchy and inverse-gamma distributions were used as hyperpriors for scaling parameters. Fixed values of hyperparameters correspond to the delta function hyperprior.

In this experiment we used the neural network with the following structure:

$\bullet$ Dense layer with $5$ units and \textit{tanh} activation;

$\bullet$ Dense layer with $5$ units and \textit{tanh} activation;

$\bullet$ Dense layer with $2$ units and \textit{softmax} as activation.

Different values of the noise parameter for data generation were tried. As expected it is hard to reconstruct the initial pattern with high noise parameter (the true class distribution could be seen in the fig.\ref{pic: ex_data}). To detect the influence of priors and hyperpriors we decided to use the large noise parameter. In the picture below the best result for circles dataset is shown ($700/300$ samples for training/validation). Here the first plot illustrates the posterior probability for the first class. The second plot illustrates model uncertainty in predictions. The uncertainty is obtained as standard deviation of prediction over batch of samples from posterior distribution.

\begin{minipage}[t]{0.32\columnwidth}
	\vspace{0.9cm}
	\textbf{Prior:} Cauchy\\
	\textbf{Hyperprior:} \\Inverse-Gamma\\
	\textbf{Accuracy:} 0.735
\end{minipage}
\begin{minipage}[t]{0.32\columnwidth}
	\centering 
	\textbf{Posterior}\\\textbf{Probability}
	\vspace{-0.3cm}
	\includegraphics[height=4cm]{pres_pics/circles_inv_cauchy_ppm}
\end{minipage}
\begin{minipage}[t]{0.32\columnwidth}
	\centering
	\vspace{0.06cm}
	\textbf{Uncertainty}
	
	\vspace{0.33cm}
	
	\includegraphics[height=4cm]{pres_pics/circles_inv_cauchy_uncert}
\end{minipage}

\vspace{\baselineskip}

In the plots below one could see the same results for moons dataset. Both datasets shows that our model become uncertain about predictions near the true decision boundary.

\begin{minipage}[t]{0.32\columnwidth}
	\vspace{0.9cm}
	\textbf{Prior:} Normal \\
	\textbf{Hyperprior:} \\Inverse-Gamma\\
	\textbf{Accuracy:} 0.851
\end{minipage}
\begin{minipage}[t]{0.32\columnwidth}
	\centering 
	\textbf{Posterior}\\\textbf{Probability}
	\vspace{-0.3cm}
	\includegraphics[height=4cm]{pres_pics/moons_inv_gauss_ppm}
\end{minipage}
\begin{minipage}[t]{0.32\columnwidth}
	\centering
	\vspace{0.06cm}
	\textbf{Uncertainty}
	
	\vspace{0.33cm}
	
	\includegraphics[height=4cm]{pres_pics/moons_inv_gauss_uncert}
\end{minipage}

\vspace{\baselineskip}

Next experiment compares the results for fixed values of hyperparameters and hierarchical bayesian approach with introducing hyperpriors. 
The following pictures shows one of the example for circles dataset.
The hyperpriors allows to find the appropriate hyperparameters values for normal and laplace priors. The fixed values were tuned on the logarithmic grid. 

\begin{minipage}[t]{0.32\columnwidth}
	\vspace{0.6cm}
	\textbf{Hyperprior:} \\Fixed values
\end{minipage}
\begin{minipage}[t]{0.32\columnwidth}
	\centering
	\textbf{Prior:} Normal
	
	\vspace{0.1cm}
	
	\includegraphics[height=4cm]{pres_pics/p_gauss}
\end{minipage}
\begin{minipage}[t]{0.32\columnwidth}
	\centering
	\textbf{Prior:} Laplace
	\vspace{-0.075cm}
	\includegraphics[height=4cm]{pres_pics/p_laplace}
\end{minipage}

\vspace{0.5cm}

\begin{minipage}[t]{0.355\columnwidth}
	\vspace{-3.6cm}
	\textbf{Hyperprior:} \\Inverse-Gamma
\end{minipage}
\begin{minipage}[t]{0.32\columnwidth}
	\includegraphics[height=4cm]{pres_pics/h_gauss}
\end{minipage}
\begin{minipage}[t]{0.32\columnwidth}
	\includegraphics[height=4cm]{pres_pics/h_laplace}
\end{minipage}

\vspace{\baselineskip}

Laplace prior is equivalent to $\ell1$ regularazation which gives sparsity solution. Let us have a look at the distributions of the weights from one of the layers from the network. In the fig.~\ref{pic::sparsity} the posterior distributions of the last dense layer weights are illustrated. We have laplace prior on the left plot and cauchy on the right. All weights except two are distributed around zero for laplace prior. It leads to the sparse solution. In contrast, the cauchy prior gives the parameters without centering around zero.

\begin{figure}[h]
	\centering
	\subfloat{\includegraphics[width=0.49\columnwidth]{pres_pics/trace_laplace_w3}}
	\hfill
	\subfloat{\includegraphics[width=0.49\columnwidth]{pres_pics/trace_cauchy_w3}}
	\caption{Estimated posterior distributions of the weights from the last layer for laplace prior (left), and cauchy prior (right)}
	\label{pic::sparsity}
\end{figure}

To summarize all the results for synthetic datasets, we formulate several \textbf{conclusions:}

$\bullet$ variational inference allows to approximate posterior
distribution;

$\bullet$ posterior distribution helps to make conclusions about uncertainties;

$\bullet$ hyperpriors are almost always give better results, especially on noisy data.

\subsection*{Uncertainty in predictions}

In this experiment we used MNIST datasets (50000/10000 objects for training/validation). The main goal was to use ADVI method on real data with several classes and analyze the behaviour of the model. Here we used a network with the following structure:

\vspace{\baselineskip}

\begin{minipage}[t]{0.64\columnwidth}
	\vspace{-6cm}
	$\bullet$ Convolution layer with 32 filters of the size $5\times 5$ and \textit{tanh} nonlinearity;
	
	$\bullet$ Max pooling layer where pool size is equal to $2\times2$;
	
	
	$\bullet$ Convolution layer with 32 filters of the size $5\times 5$ and \textit{tanh} nonlinearity;
	
	$\bullet$ Max pooling layer where pool size is equal to $2\times2$;
	
	
	$\bullet$ Dense layer with 64 units of the layer and \textit{tanh} as activation function;
	
	$\bullet$ Dense layer with 10 units of the layer and \textit{softmax}.
\end{minipage}
\hspace{0.3cm}
\begin{minipage}[t]{0.3\columnwidth}
	\includegraphics[width=1\columnwidth]{pres_pics/MNIST}
\end{minipage}


\vspace{\baselineskip}

Using the estimated posterior distributions of the considered neural network parameters we generated $100$ samples from it and computed the output of the network.
In the fig.~\ref{pic: var_eer} the first plot illustrates the variances of predictions over all samples. One could have these variances averaged for each class over misclassified objects (blue line), objects with correct prediction (green line) and all objects (red line). The variances for misclassified objects are much higher than for correct. Also one could see that for digits 2, 7, 8, 9 the results are more dispersed. The second plot shows the histogram of the expected error rate which expresses the fraction of samples for which the prediction is different from the mode of the posterior distribution.

\begin{figure}[h]
	\centering
	\subfloat{\includegraphics[width=0.49\columnwidth]{pres_pics/mnist_variance}}
	\hfill
	\subfloat{\includegraphics[width=0.49\columnwidth]{pres_pics/mnist_eer}}
	\caption{Variance over classes and expected error rate}
	\label{pic: var_eer}
\end{figure}

The fig.~\ref{pic: high_conf} demonstrates the digits from validation sets for which the expected error rate equals zero, but the model makes mistake. It means that our model is overconfident about prediction, but it is not correct. There are exactly three of them. 

\begin{figure}[h]
	\centering
	\includegraphics[width=0.74\columnwidth]{pres_pics/high_conf}
	\caption{Misclassified pictures with zero expected error rate}
	\label{pic: high_conf}
\end{figure}
\vspace{-0.2cm}

The next picture (see fig.~\ref{pic: high_conf}) shows three digits for which the expected error rate is the highest (about 0.7). For these objects our model is not confident at all. Also we show the true labels and model predictions for these examples.

The model accuracy for this dataset is $97.7 \%$. We used the different setup for priors and hypepriors. But the final model examples shown above includes normal prior with fixed values of hyperparameters.

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.74\columnwidth]{pres_pics/low_conf}
	\caption{Pictures with the lowest confidence}
	\label{pic: low_conf}
\end{figure}

\subsection*{Transfer learning}

The bayesian approach allows to use transfer learning approach to build a model on new object recognition data set using model, pretrained on another dataset. In order to implement this idea one could place informed priors centered around weights retrieved from this pretrained network. We tried this approach for two real datasets. The first one is CIFAR-10 dataset \footnote{%
	\href{https://www.cs.toronto.edu/~kriz/cifar.html}{CIFAR-10 dataset~--- https://www.cs.toronto.edu/~kriz/cifar.html}}
which consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. The second one is Cats-vs-Dogs\footnote{%
	\href{https://www.kaggle.com/c/dogs-vs-cats}{Cats-vs-Dogs dataset~--- https://www.kaggle.com/c/dogs-vs-cats}} which consists of 6000 colour images of cats and dogs with diferent sizes. There are 5000 training images and 1000 test images.

Firstly, we tried to train model with the network which were used for MNIST dataset. But we didn't get the good accuracy. 

The pretrained model that we used in this experiment was VGG-19\footnote{%
\href{ http://www.robots.ox.ac.uk/~vgg/research/very_deep/}{VGG-19 model~---  http://www.robots.ox.ac.uk/$\sim$vgg/research/very\_deep/}}. The architecture for VGG-19 model is given in fig.~\ref{pic::VGG19}. We trained our models on AWS Amazon server\footnote{%
\href{ https://aws.amazon.com/}{https://aws.amazon.com/}} on g2.2xlarge. Unfortunately, our approach didn't have enough memory in this setup for both datasets. Then we tried to cut off last layers to reduce number of estimated parameters. When we got rid of all dense layers we managed to compile the model, but it would took days to fit the model. Since that, we didn't get the results for these datasets.
\begin{figure}
\centering
\includegraphics[width=0.9\columnwidth]{pres_pics/VGG19}
\caption{Architecture of VGG-19 neural network}
\label{pic::VGG19}
\end{figure}

\section*{Conclusion}
In this project we investigated bayesian approach for deep learning models. We read the latest papers on this issue and understood the theory behind them. We also wrote implementation of bayesian deep model on python and conducted experiments using both synthetic and real data. We played with different configurations and parameters of the model and we drew conclusions which illustrate advantages and disadvantages of presented methods.
\newpage

\section*{Contributions}
\textbf{Ilya Zharikov} programmed main framework for placing priors in the network, built neural networks architectures, implemented experiments on CIFAR-10 and cats-vs-dogs datasets.

\textbf{Roman Isachenko} extended the bayesian approach to hierarchical modelling, built VGG-19 network with transfer learning approach.

\textbf{Artem Bochkarev} conducted experiments and drew conclusions from synthetic data and MNIST dataset.

Everyone were equally involved in creating presentation, writing this report, as well as general project development.

\bibliographystyle{unsrt}
\bibliography{ref.bib}
\end{document}