\documentclass[a4paper,14pt]{article} 
% Этот шаблон документа разработан в 2014 году 
% Данилом Фёдоровых (danil@fedorovykh.ru) 
% для использования в курсе 
% «Документы и презентации в \LaTeX», записанном НИУ ВШЭ 
% для Coursera.org: http://coursera.org/course/latex . 
% Исходная версия шаблона —- 
% https://www.writelatex.com/coursera/latex/5.3 

% В этом документе преамбула 

%%% Работа с русским языком 
\usepackage{cmap} % поиск в PDF 
\usepackage{mathtext} % русские буквы в формулах 
\usepackage[T2A]{fontenc} % кодировка 
\usepackage[cp1251]{inputenc} % кодировка исходного текста 
\usepackage[english]{babel} % локализация и переносы 
\usepackage{indentfirst} 
\frenchspacing
\usepackage[shortlabels]{enumitem}

\renewcommand{\epsilon}{\ensuremath{\varepsilon}} 
\renewcommand{\phi}{\ensuremath{\varphi}} 
\renewcommand{\kappa}{\ensuremath{\varkappa}} 
\renewcommand{\le}{\ensuremath{\leqslant}} 
\renewcommand{\leq}{\ensuremath{\leqslant}} 
\renewcommand{\ge}{\ensuremath{\geqslant}} 
\renewcommand{\geq}{\ensuremath{\geqslant}} 
\renewcommand{\emptyset}{\varnothing} 
\newcommand{\T}{{\text{\footnotesize\sffamily\upshape\mdseries T}}}


%%% Дополнительная работа с математикой 
\usepackage{amsmath,amsfonts,amssymb,amsthm,mathtools, bm} % AMS 
\usepackage{icomma} % "Умная" запятая: $0,2$ —- число, $0, 2$ —- перечисление 

%% Номера формул 
%\mathtoolsset{showonlyrefs=true} % Показывать номера только у тех формул, на которые есть \eqref{} в тексте. 
%\usepackage{leqno} % Нумереация формул слева 

%% Свои команды 
\DeclareMathOperator{\sgn}{\mathop{sgn}}
\DeclareMathOperator*{\argmin}{arg\,min}

%% Перенос знаков в формулах (по Львовскому) 
%\newcommand*{\hm}[1]{#1\nobreak\discretionary{} 
%	{\hbox{$\mathsurround=0pt #1$}}{}} 

%%% Работа с картинками 
\usepackage{graphicx} % Для вставки рисунков 
\setlength\fboxsep{3pt} % Отступ рамки \fbox{} от рисунка 
\setlength\fboxrule{1pt} % Толщина линий рамки \fbox{} 
\usepackage{wrapfig} % Обтекание рисунков текстом 
\usepackage{float}
\usepackage{subfig}

%%% Работа с таблицами 
\usepackage{array,tabularx,tabulary,booktabs} % Дополнительная работа с таблицами 
\usepackage{longtable} % Длинные таблицы 
\usepackage{multirow} % Слияние строк в таблице 

%%% Теоремы 
\theoremstyle{plain} % Это стиль по умолчанию, его можно не переопределять. 
\newtheorem{theorem}{Теорема}[section] 
\newtheorem{proposition}[theorem]{Утверждение}

\theoremstyle{definition} % "Определение" 
\newtheorem{corollary}{Следствие}[theorem] 
\newtheorem{problem}{Задача}[section] 

\theoremstyle{remark} % "Примечание" 
\newtheorem*{nonum}{Решение} 

%%% Программирование 
\usepackage{etoolbox} % логические операторы

%%% Страница 
\usepackage{extsizes} % Возможность сделать 14-й шрифт 
\usepackage{geometry} % Простой способ задавать поля 
\geometry{top=20mm} 
\geometry{bottom=25mm} 
\geometry{left=20mm} 
\geometry{right=20mm} 
% 
%\usepackage{fancyhdr} % Колонтитулы 
% \pagestyle{fancy} 
%\renewcommand{\headrulewidth}{0pt} % Толщина линейки, отчеркивающей верхний колонтитул 
% \lfoot{Нижний левый} 
% \rfoot{Нижний правый} 
% \rhead{Верхний правый} 
% \chead{Верхний в центре} 
% \lhead{Верхний левый} 
% \cfoot{Нижний в центре} % По умолчанию здесь номер страницы 

\usepackage{setspace} % Интерлиньяж 
\onehalfspacing % Интерлиньяж 1.5 
%\doublespacing % Интерлиньяж 2 
%\singlespacing % Интерлиньяж 1
\setlength{\parindent}{1cm}

\usepackage{lastpage} % Узнать, сколько всего страниц в документе. 

\usepackage{soul} % Модификаторы начертания 

\usepackage{hyperref} 
\usepackage[usenames,dvipsnames,svgnames,table,rgb]{xcolor} 
\hypersetup{ % Гиперссылки 
	unicode=true, % русские буквы в раздела PDF 
	pdftitle={Заголовок}, % Заголовок 
	pdfauthor={Автор}, % Автор 
	pdfsubject={Тема}, % Тема 
	pdfcreator={Создатель}, % Создатель 
	pdfproducer={Производитель}, % Производитель 
	pdfkeywords={keyword1} {key2} {key3}, % Ключевые слова 
	colorlinks=true, % false: ссылки в рамках; true: цветные ссылки 
	linkcolor=red, % внутренние ссылки 
	citecolor=black, % на библиографию 
	filecolor=magenta, % на файлы
	urlcolor=blue % на URL 
} 

\usepackage{csquotes} % Еще инструменты для ссылок 

%\usepackage[style=authoryear,maxcitenames=2,backend=biber,sorting=nty]{biblatex} 

\usepackage{multicol} % Несколько колонок 

\usepackage{tikz} % Работа с графикой 
\usepackage{pgfplots} 
\usepackage{pgfplotstable} 
\usepackage{bbm}
\usepackage{cite}

\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\bX}{\mathbf{X}} 

\author
{Ilya Zharikov, Roman Isachenko, Artem Bochkarev} 
\title 
{Bayesian Neural Networks}
\date{}

\begin{document} 
\maketitle
\begin{abstract}
	The recent advances in deep learning allow us to solve really hard real word problems with high accuracy. What these methods lack is interpretability of the model and uncertainty guarantees for the predictions. Bayesian framework allows to solve both this two issues, and with recent development in variational inference technique it is possible to take the best of two worlds and implement bayesian neural network. Our project is dedicated to testing different priors for neurons weights and exploring advantages and disadvantages of bayesian approach in deep learning.
\end{abstract}

\section*{Introduction}
Deep learning \cite{Goodfellow-et-al-2016} is one of the most promising and quickly advancing areas of machine learning research. Deep models proved to be superior to other algorithms in many fields such as image classification \cite{krizhevsky2012imagenet}, speech recognition \cite{dahl2012context}, unsupervised learning \cite{le2013building} or reinforcement learning \cite{mnih2015human}. Still, these models are mostly used as some black box in order to solve very difficult problems. We often want to know not only the prediction of the network on given imput, but we would like to understand the uncertainty \cite{blundell2015weight} of the model on this object. It is also desireable to be able to do transfer learning \cite{pan2010survey}, i.e. be able to obtain correct results on new types of data, using previous model as prior knowledge.

On the other side of the spectrum there is bayesian approach \cite{bishop2006pattern}. The main advantage of this framework is that we operate with parameters distribution, instead of their point estimate. This allows us to calculate uncertainties and other useful statistics. Another advantage of the bayesian approach is that we are free to select prior distribution for the parameters of our model \cite{browne2006comparison}, which may include real knowledge or some non-informative assumption. Finally, we may want to build hierarchical models because data is too complex and good priors are hard to guess.

With recent advances in computational technology and GPU it is very compelling to try mixing deep learning with bayesian framework. This would allow us to achive the same high performance of neural network, as well give us more freedom with different prior selection and interpretability of the results using uncertainty prediction. One of the obstacles of such approach is that full bayesian approach is very memory expensive, given huge parameters number (we need to store whole distribution instead of one number). What is more important, calculation of the model evidence is intractable for high-dimensional data. MCMC sampling methods \cite{wasserman2013all} like Metropolis-Hastings or Gibbs are too computationally expensive to perform on big models.

In our project we decided to implement alternative way of approximation of posterior probability, namely ADVI \cite{kucukelbir2016automatic}. The main idea of the method is that we constitute true posterior with some distribution from known parametric family and then we use optimization procedure to minimize their difference.

We conducted computational experiments on several datasets. We used PyMC3 \cite{salvatier2016probabilistic} and Lasagne \cite{lasagne} for probabilistic and deep learning frameworks respectively. We used synthetic datasets as well as MNIST hand-written digits dataset. For this datasets we constructed bayesian neural network models, optimized them with ADVI, played with different priors and drew conclusions from model uncertainty.

\section*{Problem Statement}
\subsection*{Bayesian approach}
Let suppose that $\bX$ is the observed data which comes from the unknown distribution $p(\bX)$. 
This distribution defines our model and is called model evidence. 
We assume that our model also contains latent variables $\bm{\theta}$. 
The likelihood function is given by conditional probability distribution $p(\bX| \bm{\theta})$. 
If we know the prior distribution $p(\bm{\theta})$ over hidden variables, we could use Bayes' theorem to derive the posterior distribution $p(\bm{\theta}| \bX)$
\begin{equation}
	p(\bm{\theta}| \bX) = \frac{p(\bX| \bm{\theta}) p(\bm{\theta})}{p(\bX)},
	\label{bayes_th}
\end{equation}
where the evidence function $p(\bX)$ could be obtained by integration over hidden variables
\[
	p(\bX) = \int p(\bX| \bm{\theta}) p(\bm{\theta})p(\bm{\theta}).
\]
The posterior distribution shows the transformed initial prior knowledge of $\bm{\theta}$ given the observed data $\bX$. 

The main goal of bayesian inference to compute the posterior distribution. 
In some cases we could derive the posterior in the closed form formula. 
However, it is impossible for complex models where the latent variables lies in high-dimensional space. 
The problem is the denominator of~\eqref{bayes_th}, since it is obtained by integrating over all possible values of $\bm{\theta}$.

Suppose that we have the posterior distribution $p(\bm{\theta}| \bX)$, then we could use the usual Maximum A Posteriori (MAP) approach to obtain point estimate for the parameters
\[
	\bm{\theta}_{\text{MAP}} = \argmax_{\bm{\theta}} \left[\ln p(\bm{\theta}|\bX)\right] =\argmax_{\bm{\theta}} \left[\ln p(\bX|\bm{\theta}) + \ln p(\bm{\theta})\right].
\]
In the case of flat prior distribution $p(\bm{\theta}) = \text{const}$, this estimate coincides with the Maximum Likelihood Estimation (MLE) approach
\[
		\bm{\theta}_{\text{MLE}} = \argmax_{\bm{\theta}} \left[\ln p(\bX|\bm{\theta})\right].
\]

\subsection*{Variational inference}

One of the widely used approach to get posterior distribution is sampling methods such as Metropolis-Hastings, Gibbs, NUTS algorithms. 
The general idea behind sampling methods is to obtain procedure for generating samples from the true posterior distribution. 
Most of these methods is based on Monte Carlo Markov Chains (MCMC) procedures. 

\bibliographystyle{unsrt}
\bibliography{ref.bib}
\end{document}