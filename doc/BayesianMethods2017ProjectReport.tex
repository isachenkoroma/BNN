\documentclass[a4paper,14pt]{article} 
% Этот шаблон документа разработан в 2014 году 
% Данилом Фёдоровых (danil@fedorovykh.ru) 
% для использования в курсе 
% «Документы и презентации в \LaTeX», записанном НИУ ВШЭ 
% для Coursera.org: http://coursera.org/course/latex . 
% Исходная версия шаблона —- 
% https://www.writelatex.com/coursera/latex/5.3 

% В этом документе преамбула 

%%% Работа с русским языком 
\usepackage{cmap} % поиск в PDF 
\usepackage{mathtext} % русские буквы в формулах 
\usepackage[T2A]{fontenc} % кодировка 
\usepackage[cp1251]{inputenc} % кодировка исходного текста 
\usepackage[english]{babel} % локализация и переносы 
\usepackage{indentfirst} 
\frenchspacing
\usepackage[shortlabels]{enumitem}

\renewcommand{\epsilon}{\ensuremath{\varepsilon}} 
\renewcommand{\phi}{\ensuremath{\varphi}} 
\renewcommand{\kappa}{\ensuremath{\varkappa}} 
\renewcommand{\le}{\ensuremath{\leqslant}} 
\renewcommand{\leq}{\ensuremath{\leqslant}} 
\renewcommand{\ge}{\ensuremath{\geqslant}} 
\renewcommand{\geq}{\ensuremath{\geqslant}} 
\renewcommand{\emptyset}{\varnothing} 
\newcommand{\T}{{\text{\footnotesize\sffamily\upshape\mdseries T}}}


%%% Дополнительная работа с математикой 
\usepackage{amsmath,amsfonts,amssymb,amsthm,mathtools, bm} % AMS 
\usepackage{icomma} % "Умная" запятая: $0,2$ —- число, $0, 2$ —- перечисление 

%% Номера формул 
%\mathtoolsset{showonlyrefs=true} % Показывать номера только у тех формул, на которые есть \eqref{} в тексте. 
%\usepackage{leqno} % Нумереация формул слева 

%% Свои команды 
\DeclareMathOperator{\sgn}{\mathop{sgn}}
\DeclareMathOperator*{\argmin}{arg\,min}

%% Перенос знаков в формулах (по Львовскому) 
%\newcommand*{\hm}[1]{#1\nobreak\discretionary{} 
%	{\hbox{$\mathsurround=0pt #1$}}{}} 

%%% Работа с картинками 
\usepackage{graphicx} % Для вставки рисунков 
\setlength\fboxsep{3pt} % Отступ рамки \fbox{} от рисунка 
\setlength\fboxrule{1pt} % Толщина линий рамки \fbox{} 
\usepackage{wrapfig} % Обтекание рисунков текстом 
\usepackage{float}
\usepackage{subfig}

%%% Работа с таблицами 
\usepackage{array,tabularx,tabulary,booktabs} % Дополнительная работа с таблицами 
\usepackage{longtable} % Длинные таблицы 
\usepackage{multirow} % Слияние строк в таблице 

%%% Теоремы 
\theoremstyle{plain} % Это стиль по умолчанию, его можно не переопределять. 
\newtheorem{theorem}{Теорема}[section] 
\newtheorem{proposition}[theorem]{Утверждение}

\theoremstyle{definition} % "Определение" 
\newtheorem{corollary}{Следствие}[theorem] 
\newtheorem{problem}{Задача}[section] 

\theoremstyle{remark} % "Примечание" 
\newtheorem*{nonum}{Решение} 

%%% Программирование 
\usepackage{etoolbox} % логические операторы

%%% Страница 
\usepackage{extsizes} % Возможность сделать 14-й шрифт 
\usepackage{geometry} % Простой способ задавать поля 
\geometry{top=20mm} 
\geometry{bottom=25mm} 
\geometry{left=20mm} 
\geometry{right=20mm} 
% 
%\usepackage{fancyhdr} % Колонтитулы 
% \pagestyle{fancy} 
%\renewcommand{\headrulewidth}{0pt} % Толщина линейки, отчеркивающей верхний колонтитул 
% \lfoot{Нижний левый} 
% \rfoot{Нижний правый} 
% \rhead{Верхний правый} 
% \chead{Верхний в центре} 
% \lhead{Верхний левый} 
% \cfoot{Нижний в центре} % По умолчанию здесь номер страницы 

\usepackage{setspace} % Интерлиньяж 
\onehalfspacing % Интерлиньяж 1.5 
%\doublespacing % Интерлиньяж 2 
%\singlespacing % Интерлиньяж 1
\setlength{\parindent}{1cm}

\usepackage{lastpage} % Узнать, сколько всего страниц в документе. 

\usepackage{soul} % Модификаторы начертания 

\usepackage{hyperref} 
\usepackage[usenames,dvipsnames,svgnames,table,rgb]{xcolor} 
\hypersetup{ % Гиперссылки 
	unicode=true, % русские буквы в раздела PDF 
	pdftitle={Заголовок}, % Заголовок 
	pdfauthor={Автор}, % Автор 
	pdfsubject={Тема}, % Тема 
	pdfcreator={Создатель}, % Создатель 
	pdfproducer={Производитель}, % Производитель 
	pdfkeywords={keyword1} {key2} {key3}, % Ключевые слова 
	colorlinks=true, % false: ссылки в рамках; true: цветные ссылки 
	linkcolor=red, % внутренние ссылки 
	citecolor=black, % на библиографию 
	filecolor=magenta, % на файлы
	urlcolor=blue % на URL 
} 

\usepackage{csquotes} % Еще инструменты для ссылок 

%\usepackage[style=authoryear,maxcitenames=2,backend=biber,sorting=nty]{biblatex} 

\usepackage{multicol} % Несколько колонок 

\usepackage{tikz} % Работа с графикой 
\usepackage{pgfplots} 
\usepackage{pgfplotstable} 
\usepackage{bbm}
\usepackage{cite}

\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\bX}{\mathbf{X}} 

\author
{Ilya Zharikov, Roman Isachenko, Artem Bochkarev} 
\title 
{Bayesian Neural Networks}
\date{}

\begin{document} 
\maketitle
\begin{abstract}
	The recent advances in deep learning allow us to solve really hard real word problems with high accuracy. What these methods lack is interpretability of the model and uncertainty guarantees for the predictions. Bayesian framework allows to solve both these two issues, and with recent development in variational inference technique it is possible to take the best of two worlds and implement bayesian neural network. Our project is dedicated to testing different priors for neurons weights and exploring advantages and disadvantages of bayesian approach in deep learning.
\end{abstract}

\section*{Introduction}
Deep learning is one of the most promising and quickly advancing areas of machine learning research~\cite{Goodfellow-et-al-2016}. Deep models proved to be superior to other algorithms in many fields such as image classification \cite{krizhevsky2012imagenet}, speech recognition~\cite{dahl2012context}, unsupervised learning~\cite{le2013building} and reinforcement learning~\cite{mnih2015human}. Still, these models are mostly used as some black box in order to solve complicated problems. We often want to know not only the prediction of the network on given input, but we would like to understand the uncertainty~\cite{blundell2015weight} of the model on this object. It is also desirable to be able to do transfer learning~\cite{pan2010survey}, i.e. be able to obtain correct results on new types of data, using previous model as prior knowledge.

On the other side of the spectrum there is bayesian approach~\cite{bishop2006pattern}. The main advantage of this framework is that we operate with parameters distribution, instead of their point estimation. This allows us to calculate uncertainties in predictions and representations. Another advantage of the bayesian approach is that we are free to select prior distribution for the parameters of our model~\cite{browne2006comparison}, which may include real knowledge or some non-informative assumption. Finally, we may want to build hierarchical models because data is too complex and good priors are hard to guess.

With recent advances in computational technology and GPU it is very compelling to try mixing deep learning with bayesian framework. This would allow us to achieve the same high performance of neural network, as well give us more freedom with different prior selection and interpretability of the results using uncertainty prediction. One of the obstacles for such approach is that full bayesian inference is very memory expensive, given huge parameters number (we need to store whole distribution instead of one point). What is more important, calculation of the model evidence is intractable for high-dimensional data. MCMC sampling methods~\cite{wasserman2013all} like Metropolis-Hastings or Gibbs are too computationally expensive to perform on big models.

In our project we decided to implement alternative way for approximation of posterior probability, namely ADVI~\cite{kucukelbir2016automatic}. The main idea of the method is that we constitute true posterior with some distribution from known parametric family and then we use stochastic optimization procedure to minimize their difference.

We conducted computational experiments on several datasets. We used\\ PyMC3~\cite{salvatier2016probabilistic} and Lasagne~\cite{lasagne} for probabilistic and deep learning frameworks respectively. We used synthetic datasets as well as MNIST hand-written digits dataset. For these datasets we constructed bayesian neural network models, optimized them with ADVI, played with different priors and drew conclusions from model uncertainty.

\section*{Problem Statement}
\subsection*{Bayesian approach}
Let suppose that $\bX$ is the observed data which comes from the unknown distribution $p(\bX)$. 
This distribution defines our model and is called model evidence. 
We assume that our model also contains latent variables $\bm{\theta}$. 
The likelihood function is given by conditional probability distribution $p(\bX| \bm{\theta})$. 
If we know the prior distribution $p(\bm{\theta})$ over hidden variables, we could use Bayes' theorem to derive the posterior distribution $p(\bm{\theta}| \bX)$
\begin{equation}
p(\bm{\theta}| \bX) = \frac{p(\bX| \bm{\theta}) p(\bm{\theta})}{p(\bX)},
\label{bayes_th}
\end{equation}
where the evidence function $p(\bX)$ could be obtained by integration over hidden variables
\[
p(\bX) = \int p(\bX| \bm{\theta}) p(\bm{\theta})p(\bm{\theta}).
\]
The posterior distribution shows the transformed initial prior knowledge of $\bm{\theta}$ given the observed data $\bX$. 

The main goal of bayesian inference to compute the posterior distribution. 
In some cases we could derive the posterior in the closed form formula. 
However, it is impossible for complex models where the latent variables lies in high-dimensional space. 
The problem is the denominator of~\eqref{bayes_th}, since it is obtained by integrating over all possible values of $\bm{\theta}$.

Suppose that we have the posterior distribution $p(\bm{\theta}| \bX)$, then we could use the usual Maximum A Posteriori (MAP) approach to obtain point estimation for the parameters
\[
\bm{\theta}_{\text{MAP}} = \argmax_{\bm{\theta}} \left[\ln p(\bm{\theta}|\bX)\right] =\argmax_{\bm{\theta}} \left[\ln p(\bX|\bm{\theta}) + \ln p(\bm{\theta})\right].
\]
In the case of flat prior distribution $p(\bm{\theta}) = \text{const}$, this estimation coincides with the Maximum Likelihood Estimation (MLE) approach
\[
\bm{\theta}_{\text{MLE}} = \argmax_{\bm{\theta}} \left[\ln p(\bX|\bm{\theta})\right].
\]

\subsection*{Variational inference}

One of the widely used approach to get posterior distribution is sampling methods such as Metropolis-Hastings, Gibbs, NUTS algorithms. 
The general idea behind sampling methods is to obtain procedure for generating samples from the true posterior distribution. 
Most of these methods are based on Monte Carlo Markov Chains (MCMC) procedures.
The main disadvantages of such methods is that they are very slow for high-dimensional data.

In order to deal with high-dimensional data we could use analytical deterministic approximation of the posterior. 
Variational inference approach determines the function which is the closest to the desired posterior distribution. 
The distance is measured by Kullback-Leibler divergence
\[
\text{KL}(q||p) = -\int q(y) \ln \left[\frac{p(y)}{q(y)}\right]dy.
\]
This distance function is non-negative and equals to zero iff $q \equiv p$
\[
\text{KL}(q||p) \geq 0; \quad \text{KL}(q||p) = 0 \Leftrightarrow q \equiv p.
\]
But the problem is that we don't know the true posterior and can't minimize the Kullback-Leibler divergence explicitly. 
Variational inference approach solves the equivalent task which follows from the equation:
\begin{multline*}
\ln p(\bX) = \int q(\bm{\theta}) \ln p(\bX) d \bm{\theta} = \int q(\bm{\theta}) \ln \left[ \frac{p(\bX, \bm{\theta})}{p(\bm{\theta} | \bX)} \right] d \bm{\theta} = \\
= \int q(\bm{\theta}) \ln \left[ \frac{q(\bm{\theta})p(\bX, \bm{\theta}) }{p(\bm{\theta} | \bX)q(\bm{\theta})} \right] d \bm{\theta} = 
- \int q(\bm{\theta}) \ln \left[ \frac{p(\bm{\theta} | \bX)}{q(\bm{\theta})} \right] d \bm{\theta} + \\
+ \int q(\bm{\theta}) \ln \left[ \frac{p(\bX, \bm{\theta}) }{q(\bm{\theta})} \right] d \bm{\theta} = \text{KL}(q||p) + \text{ELBO}(q).
\end{multline*}
Here, the log model evidence function is decomposed into the Kullback-Leibler divergence and the Empirical Lower BOund (ELBO) term. 
Since the left hand side is independent of $q$ function, the minimization of $\text{KL}(q||p)$ is equivalent to the maximization of $\text{ELBO}(q)$ which we can easily compute. 
\begin{equation}
q^* = \argmin_{q} \text{KL}(q||p) = \argmax_{q} \text{ELBO}(q).
\label{minKL_maxELBO}
\end{equation}
This lower bound can be expressed in the following way
\begin{align*}
\text{ELBO}(q) &= \int q(\bm{\theta}) \ln \left[ \frac{p(\bX, \bm{\theta}) }{q(\bm{\theta})} \right] d \bm{\theta}  = \\&= \int q(\bm{\theta}) \ln p(\bX, \bm{\theta}) d \bm{\theta} - \int q(\bm{\theta}) \ln q(\bm{\theta}) d \bm{\theta} = \\
&= \mathbb{E}_{q(\bm{\theta})} \left[\ln p(\bX, \bm{\theta})  \right] - \mathbb{E}_{q(\bm{\theta})} \left[ \ln q(\bm{\theta})  \right].
\end{align*}

The joint probability distribution $p(\bX, \bm{\theta})$ is given by product of likelihood function $p(\bX)$ and prior distribution $p(\bm{\theta})$.

\subsection*{Automatic differentiation variational inference}
In [{\color{red} link to ADVI}] the Automatic Differentiation Variational Inference (ADVI) method was proposed to solve the problem~\eqref{minKL_maxELBO}. 

At the first stage ADVI method automatically transform the original constrained variables $\bm{\theta}$ to the unconstrained real-valued variables $\bm{\zeta} = T(\bm{\theta})$. ADVI then defines the corresponding variational problem on the transformed variables, that is, to minimize $\text{KL}\left(q(\bm{\zeta})|| p(\bm{\zeta}| \bX)\right)$. With this transformation, all latent variables are defined on the same space. E.\,g. if some component $\theta$ of $\bm{\theta}$ should be non-negative ($\theta \in \mathbb{R}_+$), then one could use the logarithm transformation $\zeta = T(\theta) = \log(\theta) \in \mathbb{R}$. 

The joint probability function for transformed latent variables equals 
\[
p(\bX, \bm{\zeta}) = p(\bX, T^{-1}(\bm{\zeta}) |\det J_{T^{-1}}(\bm{\zeta})|,
\]
where $J_{T^{-1}}(\bm{\zeta})$ is Jacobian of the inverse $T$ transformation.

The function $q(\bm{\zeta})$ comes from fixed parametric family. We denote
\section*{Numerical experiments}

\subsection*{Datasets}

In our research we used synthetic data for simple binary classification problem that's not linearly separable and real data such as MNIST dataset for multi-classification problem. One can see the detailed description of used data below.

\textbf{Synthetic data}.

Synthetic data is consist of two-dimensional binary classification datasets (moons and circles) that are challenging to certain algorithms, because they are not linearly separable. Also considered datasets are very useful for visualisation.

1. Moons: two interleaving half circles (see fig.\ref{pic: ex_data} (a));

2. Circles: a large circle containing a smaller circle (see fig.\ref{pic: ex_data} (b)).


\textbf{Real data}.

As for the real data was selected MNIST\footnote{%
	\href{https://www.kaggle.com/c/digit-recognizer/data}{MNIST dataset~--- https://www.kaggle.com/c/digit-recognizer/data}} dataset. 
The data files contain gray-scale images of hand-drawn digits, from zero through nine.

Each image is $28$ pixels in height and $28$ pixels in width, for a total of $784$ pixels in total (see fig.\ref{pic: ex_data} (c)). Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between $0$ and $255$, inclusive.

\begin{figure}[h]
	\centering
	\subfloat[Moons]{\includegraphics[width=0.32\columnwidth]{pres_pics/moons_ex}}
	\hfill
	\subfloat[Circles]{\includegraphics[width=0.32\columnwidth]{pres_pics/circles_ex}}
	\hfill
	\subfloat[MNIST]{\includegraphics[width=0.32\columnwidth]{pres_pics/mnist_ex}}
	\caption{Synthetic and real data examples}
	\label{pic: ex_data}
\end{figure}

\subsection*{Priors and hyper-priors influence}

On the following one can see the design of the describing experiment.

$\qquad\qquad\qquad\qquad\qquad\qquad\:\:$\textbf{Normal}$\:\:\:\:\:$\textbf{Laplace}$\:\:\:\:\:$\textbf{Cauchy}$\quad\:\:\:\:$\textbf{Flat}

\begin{minipage}[t]{0.1\columnwidth}
	\vspace{-8.0cm}
	\rotatebox{90}{\textbf{Hierarchical modeling}$\qquad\:\:\:$\textbf{Fix}}
\end{minipage}
\begin{minipage}[t]{0.23\columnwidth}
	\vspace{-7.9cm}
	Fixed values
	
	\vspace{1.5cm}
	
	Half-Normal
	
	\vspace{1.5cm}
	
	Half-Cauchy
	
	\vspace{1.4cm}
	
	Inverse-Gamma
\end{minipage}
\hfill
\begin{minipage}[t]{0.6\columnwidth}
	\includegraphics[width=0.93\columnwidth]{pres_pics/all}
\end{minipage}

So we chose different priors and combined it with different hyper-priors and using ADVI approach obtained the posterior distributions of the model parameters. In this experiment our model is a network with the following structure:

$\bullet$

\section*{Contributions}
\textbf{Ilya Zharikov} programmed main classes and routines, built project architecture. Also tried to implement experiment on CIFAR-10 and cats-vs-dogs dataset.

\textbf{Roman Isachenko} conducted experiments and drew conclusions from synthetic datasets.

\textbf{Artem Bochkarev} conducted experiments and drew conclusions from MNIST dataset.

Everyone were equally involved in creating presentation, writing this report, as well as general project development.
\bibliographystyle{unsrt}
\bibliography{ref.bib}
\end{document}