{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import pymc3 as pm\n",
    "import lasagne\n",
    "import theano.tensor as T\n",
    "\n",
    "floatX = theano.config.floatX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "def load_dataset():\n",
    "    # We first define a download function, supporting both Python 2 and 3.\n",
    "    mnist = fetch_mldata('MNIST original')\n",
    "    data = mnist['data'].reshape((70000, 1, 28, 28)).astype(np.float64)\n",
    "    target = mnist['target'].astype(np.float32)\n",
    "    # We can now download and read the training and test set images and labels.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=50000)\n",
    "    # We reserve the last 10000 training examples for validation.\n",
    "    X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
    "    y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
    "\n",
    "    # We just return all the arrays in order, as expected in main().\n",
    "    # (It doesn't matter how we do this as long as we can read them again.)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "print(\"Loading data...\")\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_shape = (None, *X_train.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PriorWeights(object):\n",
    "    def __init__(self, mode='W', prior='gauss', **params):\n",
    "        self.count = 0\n",
    "        self.prior = prior\n",
    "        self.mode = mode\n",
    "        params.setdefault('std', 1.)\n",
    "        params.setdefault('hyper', None)\n",
    "        self.params = params\n",
    "    def __call__(self, shape):\n",
    "        self.count += 1\n",
    "        if self.params['hyper'] is None:\n",
    "            std = self.params['std']\n",
    "        elif self.params['hyper'] == 'cauchy':\n",
    "            std = pm.HalfCauchy('hyper_%s%d' % (self.mode, self.count), beta=1.)\n",
    "        elif self.params['hyper'] == 'normal':\n",
    "            std = pm.HalfNormal('hyper_%s%d' % (self.mode, self.count), mu=0., sd=1.)\n",
    "        elif self.params['hyper'] == 'flat':\n",
    "            std = pm.Flat()\n",
    "        if self.prior == 'gauss':\n",
    "            return pm.Normal('%s%d' % (self.mode, self.count), mu=0, sd=std, \n",
    "                         testval=np.random.normal(size=shape).astype(np.float64),\n",
    "                         shape=shape)\n",
    "        elif self.prior == 'laplace': \n",
    "            return pm.Laplace('%s%d' % (self.mode, self.count), mu=0, b=std, \n",
    "                         testval=np.random.normal(size=shape).astype(np.float64),\n",
    "                         shape=shape)\n",
    "        elif self.prior == 'flat':\n",
    "            return pm.Flat('%s%d' % (self.mode, self.count), \n",
    "                           testval=np.random.normal(size=shape).astype(np.float64), \n",
    "                           shape=shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_ann(prior_b, prior_W, input_var, target_var, \n",
    "              input_shape, params=[176, 64]):\n",
    "    \n",
    "    n_hid1, n_hid2 = params\n",
    "    with pm.Model() as neural_network:\n",
    "        l_in = lasagne.layers.InputLayer(shape=input_shape,\n",
    "                                         input_var=input_var)\n",
    "        l_conv1 = lasagne.layers.Conv2DLayer(\n",
    "                            l_in, num_filters=32, filter_size=(5, 5),\n",
    "                            b=prior_b, W=prior_W,\n",
    "                            nonlinearity=lasagne.nonlinearities.tanh)\n",
    "        \n",
    "        l_pool1 = lasagne.layers.MaxPool2DLayer(l_conv1, pool_size=(2, 2))\n",
    "        \n",
    "        l_conv2 = lasagne.layers.Conv2DLayer(\n",
    "                            l_pool1, num_filters=32, filter_size=(5, 5),\n",
    "                            b=prior_b, W=prior_W,\n",
    "                            nonlinearity=lasagne.nonlinearities.tanh)\n",
    "\n",
    "        l_pool2 = lasagne.layers.MaxPool2DLayer(l_conv2, pool_size=(2, 2))\n",
    "\n",
    "        l_dense1 = lasagne.layers.DenseLayer(\n",
    "                            l_pool2, num_units=n_hid1,\n",
    "                            b=prior_b, W=prior_W,\n",
    "                            nonlinearity=lasagne.nonlinearities.tanh)\n",
    "\n",
    "        l_dense2 = lasagne.layers.DenseLayer(\n",
    "                            l_dense1, num_units=n_hid2,\n",
    "                            b=prior_b, W=prior_W,\n",
    "                            nonlinearity=lasagne.nonlinearities.tanh)\n",
    "\n",
    "        l_out = lasagne.layers.DenseLayer(\n",
    "                            l_dense2, num_units=10,\n",
    "                            b=prior_b, W=prior_W,\n",
    "                            nonlinearity=lasagne.nonlinearities.softmax,\n",
    "        )\n",
    "\n",
    "        prediction = lasagne.layers.get_output(l_out)\n",
    "        \n",
    "        out = pm.Categorical('out', prediction,\n",
    "                           observed=target_var,\n",
    "                           total_size=y_train.shape[0])\n",
    "    \n",
    "    return neural_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_minibatch(data):\n",
    "    rng = np.random.RandomState(0)\n",
    "    \n",
    "    while True:\n",
    "        ixs = rng.randint(len(data), size=100)\n",
    "        yield data[ixs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_prediction(samples_proba):\n",
    "    return mode(np.argmax(sample_proba(X_test, 500), \n",
    "                          axis=-1), axis=0).mode[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "minibatch_X = pm.generator(create_minibatch(X_train))\n",
    "minibatch_y = pm.generator(create_minibatch(y_train))\n",
    "neural_network_minibatch = build_ann(PriorWeights(mode='b', prior='flat'), \n",
    "                                     PriorWeights(mode='W', prior='gauss'),\n",
    "                                     minibatch_X, minibatch_y, \n",
    "                                     input_shape, params=[176, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ADVI_ITERS = 10000\n",
    "N_SAMPLES = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 97,546: 100%|██████████| 10000/10000 [25:54<00:00,  6.40it/s]     \n",
      "Finished [100%]: Average Loss = 97,523\n"
     ]
    }
   ],
   "source": [
    "with neural_network_minibatch:\n",
    "    inference = pm.ADVI()\n",
    "    approx = pm.fit(ADVI_ITERS, method=inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
